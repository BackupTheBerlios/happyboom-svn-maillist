<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<HTML>
 <HEAD>
   <TITLE> [Happyboom-svn] r260 - haypo/mediawiki
   </TITLE>
   <LINK REL="Index" HREF="http://lists.berlios.de/pipermail/happyboom-svn/2005-November/index.html" >
   <LINK REL="made" HREF="mailto:happyboom-svn%40lists.berlios.de?Subject=Re%3A%20%5BHappyboom-svn%5D%20r260%20-%20haypo/mediawiki&In-Reply-To=%3C200511212315.jALNFJGd028084%40sheep.berlios.de%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <style type="text/css">
       pre {
           white-space: pre-wrap;       /* css-2.1, curent FF, Opera, Safari */
           }
   </style>
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="000165.html">
   <LINK REL="Next"  HREF="000184.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[Happyboom-svn] r260 - haypo/mediawiki</H1>
    <B>haypo at BerliOS</B> 
    <A HREF="mailto:happyboom-svn%40lists.berlios.de?Subject=Re%3A%20%5BHappyboom-svn%5D%20r260%20-%20haypo/mediawiki&In-Reply-To=%3C200511212315.jALNFJGd028084%40sheep.berlios.de%3E"
       TITLE="[Happyboom-svn] r260 - haypo/mediawiki">haypo at berlios.de
       </A><BR>
    <I>Tue Nov 22 00:15:19 CET 2005</I>
    <P><UL>
        <LI>Previous message: <A HREF="000165.html">[Happyboom-svn] r259 - in greycstoration_gimp: . po src
</A></li>
        <LI>Next message: <A HREF="000184.html">[Happyboom-svn] r261 - / haypo haypo/greycstoration_gimp
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#166">[ date ]</a>
              <a href="thread.html#166">[ thread ]</a>
              <a href="subject.html#166">[ subject ]</a>
              <a href="author.html#166">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>Author: haypo
Date: 2005-11-22 00:15:18 +0100 (Tue, 22 Nov 2005)
New Revision: 260

Modified:
   haypo/mediawiki/import_pages.py
Log:
Improve import.


Modified: haypo/mediawiki/import_pages.py
===================================================================
--- haypo/mediawiki/import_pages.py	2005-11-21 23:03:09 UTC (rev 259)
+++ haypo/mediawiki/import_pages.py	2005-11-21 23:15:18 UTC (rev 260)
@@ -4,7 +4,6 @@
 import sys, os, re, types
 #from elementtree.ElementTree import ElementTree
 from xml.dom.minidom import parseString
-from xml.dom.minidom import parseString
 from xml.parsers.expat import ParserCreate
 from xml import sax
 from subprocess import Popen, PIPE
@@ -18,109 +17,159 @@
     print &quot;    =&gt; will use <A HREF="http://www.haypocalc.com/wiki/Special:Allpages">http://www.haypocalc.com/wiki/Special:Allpages</A>&quot;
     sys.exit(1)
 
-def escapeshellarg(arg):
-    # TODO (use os.&lt;the right function&gt;)
-    arg = arg.replace(&quot;\\&quot;, &quot;\\\\&quot;).replace(&quot;'&quot;, &quot;\\'&quot;)
-    return &quot;'&quot;+arg+&quot;'&quot;
-
 def error(msg):
     print msg
 
 def warning(msg):
     print msg
 
-def download(url, params={}):
-    params = urllib.urlencode(params)
-    f = urllib.urlopen(url, params)
-    return f.read()
+class Importer:
+    def __init__(self, base_url):
+        # Add &quot;/&quot; prefix to url if needed
+        if base_url[-1] != &quot;/&quot;:
+            self.base_url = base_url+&quot;/&quot;
+        else:
+            self.base_url = base_url
 
-def getAllpages(url, namespace):
-    pages = download(url+&quot;Special:Allpages&quot;, {&quot;namespace&quot;: namespace})
-    if pages == None:
-        return None
+        self.only_last = False
+        self.dir = &quot;pages&quot;
+        self.use_gzip = True
+        self.max_per_file = 200 
+        
+    def download(self, page, params={}):
+        params = urllib.urlencode(params)
+        f = urllib.urlopen(self.base_url+page, params)
+        return f.read()
 
-    # Work around MediaWiki 1.4 bug (don't escape &quot;&amp;&quot; chararacter)
-    pages = re.sub(&quot;&amp;([^a-zA-Z])&quot;, r&quot;&amp;\1&quot;, pages)
+    def getAllpages(self, namespace):
+        pages = self.download(&quot;Special:Allpages&quot;, {&quot;namespace&quot;: namespace})
+        if pages == None:
+            return None
 
-    xml = parseString(pages)
-    root = xml.documentElement
-    tables = root.getElementsByTagName('table')
-    assert len(tables) == 2
-    table = tables[1]
-    links = table.getElementsByTagName('a')
-    pages = []
-    for link in links:
-        page = link.getAttribute('href').encode(&quot;ASCII&quot;)
-        page = page.split(&quot;/&quot;)
-        page = urllib.unquote(page[-1])
-        if type(page) == types.UnicodeType:
-            page = page.encode(&quot;UTF-8&quot;)            
-        pages.append( page )
-    return pages
+        # Work around MediaWiki 1.4 bug (don't escape &quot;&amp;&quot; chararacter)
+        pages = re.sub(&quot;&amp;([^a-zA-Z])&quot;, r&quot;&amp;\1&quot;, pages)
 
-def importNamespace(url, dir, namespace, only_last):
-    print &quot;Download document list.&quot;
-    pages = getAllpages(url, namespace)
-    if pages == None:
-        print &quot;Error: Can't get document list.&quot;
-        sys.exit(1)
-    if len(pages) == 0:
-        print &quot;(empty)&quot;
-        return
-    print &quot;Download XML data (%u documents) ...&quot; % len(pages)
-    args = { \
-        &quot;action&quot;: &quot;submit&quot;,
-        &quot;pages&quot;: &quot;\n&quot;.join(pages)
-    }
-    if only_last:
-        args[&quot;curonly&quot;] = &quot;on&quot;
-    data = download(url+&quot;Special:Export&quot;, args)
-    if data == None:
-        error(&quot;Fail to download namespace %s.&quot; % namespace)
-        sys.exit(1)
-    filename = os.path.join(dir, &quot;namespace_%s.xml&quot; % namespace)
-    f = open(filename, 'w')
-    f.write(data)
-    f.close()
+        xml = parseString(pages)
+        root = xml.documentElement
+        tables = root.getElementsByTagName('table')
+        assert len(tables) == 2
+        table = tables[1]
+        links = table.getElementsByTagName('a')
+        pages = []
+        for link in links:
+            page = link.getAttribute('href').encode(&quot;ASCII&quot;)
+            page = page.split(&quot;/&quot;)
+            page = urllib.unquote(page[-1])
+            if type(page) == types.UnicodeType:
+                page = page.encode(&quot;UTF-8&quot;)            
+            pages.append( page )
+        return pages
 
-def importPages(url, only_last=False):
-    # Add &quot;/&quot; prefix to url if needed
-    if url[-1] != &quot;/&quot;:
-        url = url + &quot;/&quot;
+    def savePages(self, namespace, part, content):
+        if part != None:
+            filename = &quot;namespace_%s_part%u.xml&quot; % (namespace, part)
+        else:
+            filename = &quot;namespace_%s.xml&quot; % namespace
+        filename = os.path.join(self.dir, filename)
+        if self.use_gzip:
+            filename = filename + &quot;.gz&quot;
+            f = GzipFile(filename, 'w')
+            f.write(content)
+            f.close()
+        else:
+            f = open(filename, 'w')
+            f.write(content)
+            f.close()
+        print &quot;saved to %s&quot; % (filename)
 
-    # Create pages/ if needed
-    dir = &quot;pages&quot;
-    try:
-      os.mkdir(dir)
-    except OSError, err:
-        if err[0] != 17:
-            raise
+    def importPages(self, namespace, pages, part):
+        args = { \
+            &quot;action&quot;: &quot;submit&quot;,
+            &quot;pages&quot;: &quot;\n&quot;.join(pages)
+        }
+        if self.only_last:
+            args[&quot;curonly&quot;] = &quot;on&quot;
+        data = self.download(&quot;Special:Export&quot;, args)
+        if data == None:
+            error(&quot;Fail to download namespace %s.&quot; % namespace)
+            sys.exit(1)
+        self.savePages(namespace, part, data)
 
-    # Which namespaces have to be downloaded?
-    namespaces = range(0,16)
-    namespaces.remove(8) # MediaWiki
-    namespaces.remove(9) # Talk:MediaWiki
-    
-    # Download each namespace
-    i = 1
-    for namespace in namespaces: 
-        print &quot;================== Import namespace %u (%u/%u) ============== &quot; \
-                % (namespace, i, len(namespaces))
-        importNamespace(url, dir, namespace, only_last)
-        i = i + 1
+    def importNamespace(self,  namespace):
+        print &quot;Download document list.&quot;
+        pages = self.getAllpages(namespace)
+        if pages == None:
+            print &quot;Error: Can't get document list.&quot;
+            sys.exit(1)
+        if len(pages) == 0:
+            print &quot;(empty)&quot;
+            return
+        part = 1
+        position = 1
+        total = len(pages)
+        while len(pages) != 0:
+            if self.max_per_file != 0:
+                sub = pages[:self.max_per_file]
+                pages = pages[self.max_per_file:]
+            else:
+                sub = pages
+                pages = []
+            sys.stdout.write(&quot;Download document(s) %u..%u / %u ... &quot; \
+                 % (position, position+len(sub)-1, total))
+            sys.stdout.flush()
+            try:
+                if total != len(sub):
+                    self.importPages(namespace, sub, part)
+                    part = part + 1
+                    position = position + len(sub)
+                else:
+                    part = None
+                    self.importPages(namespace, sub, part)
+            except:
+                sys.stdout.write(&quot;ERROR!\n\n&quot;)
+                raise
 
-def exportPages(url):
-    print &quot;TODO&quot;
+    def doImport(self):
+        # Which namespaces have to be downloaded?
+        namespaces = range(0,16)
+        namespaces.remove(8) # MediaWiki
+        namespaces.remove(9) # Talk:MediaWiki
 
-def main():
-    if len(sys.argv) != 2:
-        usage()
-    url = sys.argv[1]
-    try:
-        importPages(url)
-    except KeyboardInterrupt:
-        print &quot;Interrupted (CTRL+C): be carefull, download incomplete!&quot;
-        sys.exit(1)
+        # Sum up before starting
+        print &quot;Base url: %s&quot; % (self.base_url)
+        print &quot;Store into gzip: %s&quot; % (self.use_gzip)
+        print &quot;Max documents/file: %s&quot; % (self.max_per_file)
 
-if __name__==&quot;__main__&quot;:
-    main()        
+        # Create pages/ if needed
+        if not os.access(self.dir, os.F_OK):
+            print &quot;Create subdirectory %s&quot; % (self.dir)
+            os.mkdir(self.dir)
+        print &quot;&quot;
+        
+        # Download each namespace
+        i = 1
+        for namespace in namespaces: 
+            namespace = 6
+            print &quot;================== Import namespace %u (%u/%u) ============== &quot; \
+                    % (namespace, i, len(namespaces))
+            self.importNamespace(namespace)
+            return
+            i = i + 1
+
+if len(sys.argv) != 2:
+    usage()
+base_url = sys.argv[1]
+try:
+    from gzip import GzipFile
+    use_gzip = True
+except ImportError:
+    warning(&quot;Can't load gzip module.&quot;)
+    use_gzip = False
+try:
+    my = Importer(base_url)
+    my.use_gzip = use_gzip
+    my.doImport()
+except KeyboardInterrupt:
+    print &quot;Interrupted (CTRL+C): be carefull, download incomplete!&quot;
+    sys.exit(1)
+print &quot;&quot;    


</PRE>

<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="000165.html">[Happyboom-svn] r259 - in greycstoration_gimp: . po src
</A></li>
	<LI>Next message: <A HREF="000184.html">[Happyboom-svn] r261 - / haypo haypo/greycstoration_gimp
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="date.html#166">[ date ]</a>
              <a href="thread.html#166">[ thread ]</a>
              <a href="subject.html#166">[ subject ]</a>
              <a href="author.html#166">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="https://lists.berlios.de/mailman/listinfo/happyboom-svn">More information about the Happyboom-svn
mailing list</a><br>
</body></html>
